{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Felix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_real = open('real/sam_content.txt', 'r', encoding='utf-8')\n",
    "file_fake = open('fake/sam_content_phoney.txt', 'r', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for line in file_real:\n",
    "    data.append(line)\n",
    "    labels.append(1)\n",
    "for line in file_fake:\n",
    "    data.append(line)\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(data, labels, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4948554630083293, 0.5051445369916707]\n"
     ]
    }
   ],
   "source": [
    "pi = [ sum(train_y)/len(train_y), sum([1 for x in train_y if x == 0])/len(train_y)]\n",
    "print(pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_x)\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x).todense()\n",
    "xvalid_count =  count_vect.transform(valid_x).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFreq = pd.DataFrame(columns=['words','class1','class2'])\n",
    "wordFreq['words'] = count_vect.get_feature_names()\n",
    "\n",
    "x_train_class1 = xtrain_count[train_y==0]\n",
    "x_train_class2 = xtrain_count[train_y==1]\n",
    "\n",
    "count_class1 = np.sum(x_train_class1,axis=0)\n",
    "count_class2 = np.sum(x_train_class2,axis=0)\n",
    "\n",
    "vocab_size1 = len(np.where(count_class1==0)[1])\n",
    "vocab_size2 = len(np.where(count_class2==0)[1])\n",
    "\n",
    "alpha=10\n",
    "count_class1 = np.array( (count_class1+alpha) /(np.sum(count_class1)+vocab_size1 +1))\n",
    "count_class2 = np.array( (count_class2+alpha) /(np.sum(count_class2)+vocab_size2 +1))\n",
    "\n",
    "wordFreq['class1'] = pd.Series(count_class1.ravel())\n",
    "wordFreq['class2'] = pd.Series(count_class2.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is: 0.5634492895639392\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(xtrain_count))\n",
    "for i in range(len(xtrain_count)):\n",
    "    idx = np.where(xtrain_count[i,:]!=0)[1]\n",
    "    lh1 = wordFreq['class1'].iloc[idx].prod()\n",
    "    lh2 = wordFreq['class2'].iloc[idx].prod()\n",
    "    posterior1 = lh1*pi[0]\n",
    "    posterior2 = lh2*pi[1]\n",
    "\n",
    "    if posterior1>posterior2:\n",
    "        train_preds[i] = 0\n",
    "    else:\n",
    "        train_preds[i] = 1\n",
    "\n",
    "\n",
    "matches = np.sum(train_y==train_preds)\n",
    "print('Train accuracy is: ' + str(matches/len(train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.5901826484018264\n"
     ]
    }
   ],
   "source": [
    "valid_preds = np.zeros(len(xvalid_count))\n",
    "for i in range(len(xvalid_count)):\n",
    "    idx = np.where(xvalid_count[i,:]!=0)[1]\n",
    "    lh1 = wordFreq['class1'].iloc[idx].prod()\n",
    "    lh2 = wordFreq['class2'].iloc[idx].prod()\n",
    "    posterior1 = lh1*pi[0]\n",
    "    posterior2 = lh2 * pi[1]\n",
    "\n",
    "    if posterior1>posterior2:\n",
    "        valid_preds[i] = 0\n",
    "    else:\n",
    "        valid_preds[i] = 1\n",
    "    temp = 1\n",
    "\n",
    "matches = np.sum(valid_y==valid_preds)\n",
    "print('Validation accuracy is: '+str(matches/len(valid_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
